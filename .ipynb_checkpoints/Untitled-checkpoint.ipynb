{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# nltk module\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "# module we need for Tensorflow\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# load intents\n",
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pickle to load  data\n",
    "data = pickle.load( open( \"./pickle/ahsbot-data.pkl\", \"rb\" ) )\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "\n",
    "#Use pickle to load in the pre-trained model\n",
    "with open('./pickle/ahsbot-model.pkl', 'rb') as f:\n",
    "    global model\n",
    "    model = pickle.load(f)\n",
    "    \n",
    "confidence_level = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)\n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sentence):\n",
    "    # Add below two lines for workaround error _thread._local' object has no attribute 'value'\n",
    "    import keras.backend.tensorflow_backend as tb\n",
    "    tb._SYMBOLIC_SCOPE.value = True\n",
    "   \n",
    "    global model\n",
    "    global confidence_level\n",
    "    ERROR_THRESHOLD = confidence_level\n",
    "    # generate probabilities from the model\n",
    "    input_data = pd.DataFrame([bow(sentence, words)], dtype=float, index=['input'])\n",
    "   \n",
    "    results = model.predict([input_data],workers=1,verbose=1)[0]\n",
    "  \n",
    "    # filter out predictions below a threshold, and provide intent index\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], str(r[1])))\n",
    "    # return tuple of intent and probability \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def botResponse(sentence):\n",
    "    import random\n",
    "    tagResult = classify(sentence)\n",
    "    if not tagResult:\n",
    "        return \"Sorry, I do not understand\"\n",
    "    responseList = list(filter(lambda x:x['tag'] == tagResult[0][0],intents['intents']))\n",
    "    return random.choice(responseList[0]['responses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Can I use EMA with .NET?\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Bot: Offering support for Adverse drug reaction, Blood pressure, Hospitals and Pharmacies\n"
     ]
    }
   ],
   "source": [
    "message='Can I use EMA with .NET?'\n",
    "print(\"User:\",message)\n",
    "print(\"Bot:\",botResponse(message))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
